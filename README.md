# ğŸ¥ Videoâ€“Text Captioning using VLLM with Streamlit Interface

This project demonstrates **Visionâ€“Language understanding** for video data using **BLIP (Bootstrapped Language-Image Pretraining)** â€” a **VLLM (Visionâ€“Language Large Model)**.  
The system automatically extracts frames from videos and generates **descriptive text captions**, showing how **deep learning models align heterogeneous modalities** such as vision and language.

---

## ğŸ¯ Objective
To explore **cross-modal embeddings** and **attention-based multimodal architectures** that align image and text representations for video understanding.  
This work directly aligns with the **Multimodal Analysis and Retrieval** research focus.

---

## ğŸ§© Features
- ğŸ§  Uses pre-trained **BLIP model** from Salesforce (Hugging Face)
- ğŸ¥ Samples frames automatically from input video
- ğŸ“ Generates frame-wise and aggregated captions
- âš™ï¸ Works fully on **CPU or GPU**
- ğŸ’¡ Demonstrates **VLLM, attention mechanisms, and cross-modal feature alignment**

---

**ğŸ§  Model Details**

**Model:** Salesforce/blip-image-captioning-base

**Architecture:** Vision Transformer (ViT) + GPT-like text decoder

**Concepts Demonstrated:**

Cross-modal feature alignment

Attention mechanisms in multimodal transformers

Visionâ€“Language Large Models (VLLM) for caption generation

---

**ğŸ§  Learning Outcomes**

Implemented a Visionâ€“Language model for real-world video understanding

Learned multimodal data processing and attention-based captioning

Explored temporal reasoning using sampled frame aggregation

---

## ğŸ—‚ï¸ Folder Structure

Video-Text-Captioning-VLLM/
â”‚
â”œâ”€â”€ app_video_caption.py # Main CLI captioning script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ sample_data/ # Input videos
â”‚ â””â”€â”€ factory.mp4
â”œâ”€â”€ results/ # Output frames + captions
â”‚ â”œâ”€â”€ frame_01.jpg
â”‚ â”œâ”€â”€ frame_01.txt
â”‚ â””â”€â”€ ...
â””â”€â”€ README.md

---


---

## âš™ï¸ Installation and Usage

### 1ï¸âƒ£ Create Environment
```bash
python -m venv venv
venv\Scripts\activate        # Windows
# or
source venv/bin/activate     # macOS/Linux

2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt

3ï¸âƒ£ Run Captioning
```bash
python app_video_caption.py --video sample_data/factory.mp4 --sample_fps 1 --max_frames 12

---

ğŸ“Š Example Output

Terminal Output

Per-frame captions:
 [1] a man working on a piece of wood
 [2] a man working on a piece of furniture
 [3] a man working on a machine in a room
 ...
===== Final aggregated caption =====
a man working on a table in a kitchen

Result Folder

results/
â”œâ”€â”€ frame_01.jpg
â”œâ”€â”€ frame_01.txt
â”œâ”€â”€ frame_02.jpg
â”œâ”€â”€ frame_02.txt
...


